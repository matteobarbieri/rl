{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI gym frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GAMES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds_p = np.linspace(-1.1,0.5, 20)\n",
    "# thresholds_v = np.linspace(-0.06,0.06, 20)\n",
    "\n",
    "def discretize_state(s_cont, env):\n",
    "    \n",
    "#     s_disc = (\n",
    "#         np.digitize(s_cont[0], thresholds_p),\n",
    "#         np.digitize(s_cont[1], thresholds_v))\n",
    "    \n",
    "    s_disc = (s_cont - env.observation_space.low)*np.array([10, 100])\n",
    "    s_disc = np.round(s_disc, 0).astype(int)\n",
    "    \n",
    "    return s_disc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_greedy(q_table, obs, env):\n",
    "    \n",
    "#     print(np.argmax(q_table[obs[0], obs[1]]))\n",
    "    \n",
    "#     1/0\n",
    "    \n",
    "    return np.argmax(q_table[obs[0], obs[1]])\n",
    "\n",
    "#     max_i = 0\n",
    "#     max_v = q_table[(obs, 0)]\n",
    "\n",
    "    \n",
    "    \n",
    "#     for i in range(env.action_space.n):\n",
    "#         v = q_table[(obs, i)]\n",
    "\n",
    "#         if v > max_v:\n",
    "#             max_v = v\n",
    "#             max_i = i\n",
    "\n",
    "#     return max_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_eps(q_table, obs, env, eps):\n",
    "    \n",
    "    # q_table[obs, a]\n",
    "    \n",
    "    if random.random() < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return choose_action_greedy(q_table, obs, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_game(env, q_table, n_games=N_GAMES):\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action greedily\n",
    "            new_obs, reward, done, info = env.step(choose_action_greedy(q_table, discretize_state(obs, env), env))\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = new_obs\n",
    "\n",
    "    return total_reward/n_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(q_table, obs, action, reward, new_obs, lr, gamma):\n",
    "    \"\"\"\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    gamma : float\n",
    "        Discount factor for future rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    # TD(0) learning\n",
    "\n",
    "    # Update entry using bellman's equation\n",
    "\n",
    "    target = reward + gamma * np.max(q_table[new_obs[0], new_obs[1]])\n",
    "    q_error = target - q_table[obs[0], obs[1], action]\n",
    "    q_table[obs[0], obs[1], action] += lr * q_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_DECAY = 0.99993\n",
    "GAMMA = 0.99\n",
    "LR = 0.2\n",
    "\n",
    "EVALUATE_EVERY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(env, q_table, n_episodes, eps, eps_decay=EPS_DECAY, evaluate_every=EVALUATE_EVERY):\n",
    "    \n",
    "    mean_rewards = list()\n",
    "    \n",
    "    total_reward = 0\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        done = False\n",
    "\n",
    "        obs = env.reset()\n",
    "        obs_d = discretize_state(obs, env)\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action epsilon-greedily\n",
    "            action = choose_action_eps(q_table, obs_d, env, eps)\n",
    "\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            new_obs_d = discretize_state(new_obs, env)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            q_learning(q_table, obs_d, action, reward, new_obs_d, LR, GAMMA)\n",
    "            obs_d = new_obs_d\n",
    "\n",
    "#         eps *= eps_decay\n",
    "        eps -= eps_decay\n",
    "\n",
    "        # Evaluate policy every N games\n",
    "        if (i+1)%evaluate_every == 0:\n",
    "\n",
    "#             test_reward = test_game(env, q_table)\n",
    "#             print(f'\\tEp: {i+1}  Average reward: {total_reward/evaluate_every} Test reward: {test_reward} {eps:.2f}')\n",
    "                \n",
    "            print(f'\\tEp: {i+1}  Average reward: {total_reward/evaluate_every} eps: {eps:.2f}')\n",
    "            \n",
    "            mean_rewards.append(total_reward/evaluate_every)\n",
    "    \n",
    "            total_reward = 0\n",
    "\n",
    "#             mean_rewards.append(test_reward)\n",
    "            \n",
    "            \n",
    "    return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEp: 100  Average reward: -200.0 eps: 0.98\n",
      "\tEp: 200  Average reward: -200.0 eps: 0.96\n",
      "\tEp: 300  Average reward: -200.0 eps: 0.94\n",
      "\tEp: 400  Average reward: -200.0 eps: 0.92\n",
      "\tEp: 500  Average reward: -200.0 eps: 0.90\n",
      "\tEp: 600  Average reward: -200.0 eps: 0.88\n",
      "\tEp: 700  Average reward: -200.0 eps: 0.86\n",
      "\tEp: 800  Average reward: -200.0 eps: 0.84\n",
      "\tEp: 900  Average reward: -200.0 eps: 0.82\n",
      "\tEp: 1000  Average reward: -200.0 eps: 0.80\n",
      "\tEp: 1100  Average reward: -200.0 eps: 0.78\n",
      "\tEp: 1200  Average reward: -200.0 eps: 0.76\n",
      "\tEp: 1300  Average reward: -200.0 eps: 0.74\n",
      "\tEp: 1400  Average reward: -200.0 eps: 0.72\n",
      "\tEp: 1500  Average reward: -200.0 eps: 0.70\n",
      "\tEp: 1600  Average reward: -200.0 eps: 0.68\n",
      "\tEp: 1700  Average reward: -200.0 eps: 0.66\n",
      "\tEp: 1800  Average reward: -200.0 eps: 0.64\n",
      "\tEp: 1900  Average reward: -200.0 eps: 0.62\n",
      "\tEp: 2000  Average reward: -200.0 eps: 0.60\n",
      "\tEp: 2100  Average reward: -200.0 eps: 0.58\n",
      "\tEp: 2200  Average reward: -200.0 eps: 0.56\n",
      "\tEp: 2300  Average reward: -200.0 eps: 0.54\n",
      "\tEp: 2400  Average reward: -200.0 eps: 0.52\n",
      "\tEp: 2500  Average reward: -200.0 eps: 0.50\n",
      "\tEp: 2600  Average reward: -200.0 eps: 0.48\n",
      "\tEp: 2700  Average reward: -200.0 eps: 0.46\n",
      "\tEp: 2800  Average reward: -200.0 eps: 0.44\n",
      "\tEp: 2900  Average reward: -200.0 eps: 0.42\n",
      "\tEp: 3000  Average reward: -200.0 eps: 0.40\n",
      "\tEp: 3100  Average reward: -200.0 eps: 0.38\n",
      "\tEp: 3200  Average reward: -200.0 eps: 0.36\n",
      "\tEp: 3300  Average reward: -199.75 eps: 0.34\n",
      "\tEp: 3400  Average reward: -199.21 eps: 0.32\n",
      "\tEp: 3500  Average reward: -198.96 eps: 0.30\n",
      "\tEp: 3600  Average reward: -198.7 eps: 0.28\n",
      "\tEp: 3700  Average reward: -198.13 eps: 0.26\n",
      "\tEp: 3800  Average reward: -198.84 eps: 0.24\n",
      "\tEp: 3900  Average reward: -199.22 eps: 0.22\n",
      "\tEp: 4000  Average reward: -195.14 eps: 0.20\n",
      "\tEp: 4100  Average reward: -198.33 eps: 0.18\n",
      "\tEp: 4200  Average reward: -197.05 eps: 0.16\n",
      "\tEp: 4300  Average reward: -194.16 eps: 0.14\n",
      "\tEp: 4400  Average reward: -185.41 eps: 0.12\n",
      "\tEp: 4500  Average reward: -184.88 eps: 0.10\n",
      "\tEp: 4600  Average reward: -196.99 eps: 0.08\n",
      "\tEp: 4700  Average reward: -176.56 eps: 0.06\n",
      "\tEp: 4800  Average reward: -187.55 eps: 0.04\n",
      "\tEp: 4900  Average reward: -182.59 eps: 0.02\n",
      "\tEp: 5000  Average reward: -188.13 eps: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Main learning loop\n",
    "\n",
    "N_EPISODES = 5000\n",
    "\n",
    "eps = 1.0\n",
    "\n",
    "# q_table = defaultdict(float)\n",
    "\n",
    "# Determine size of discretized state space\n",
    "num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1\n",
    "\n",
    "\n",
    "# Initialize Q table\n",
    "q_table = np.random.uniform(low = -1, high = 1, \n",
    "                      size = (num_states[0], num_states[1], \n",
    "                              env.action_space.n))\n",
    "\n",
    "# Save n of actions\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "mean_rewards = training(env, q_table, N_EPISODES, eps, eps_decay=eps/N_EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(mean_rewards)\n",
    "plt.title(\"Mean rewards every N Iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI gym frozenlake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GAMES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/miniconda3/envs/rl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_p = np.linspace(-1.1,0.5, 10)\n",
    "thresholds_v = np.linspace(-0.06,0.06, 10)\n",
    "\n",
    "def discretize_state(s_cont):\n",
    "    \n",
    "#     thresholds_p = np.linspace(-1.1,0.5, 10)\n",
    "#     thresholds_v = np.linspace(-0.6,0.6, 10)\n",
    "    \n",
    "#     s_disc = (s_cont*beta).astype(int)\n",
    "\n",
    "#     return tuple(s_disc)\n",
    "\n",
    "    s_disc = (\n",
    "        np.digitize(s_cont[0], thresholds_p),\n",
    "        np.digitize(s_cont[1], thresholds_v))\n",
    "    \n",
    "    return s_disc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_greedy(q_table, obs, env):\n",
    "    \n",
    "    max_i = 0\n",
    "    max_v = q_table[(obs, 0)]\n",
    "\n",
    "    for i in range(env.action_space.n):\n",
    "        v = q_table[(obs, i)]\n",
    "\n",
    "        if v > max_v:\n",
    "            max_v = v\n",
    "            max_i = i\n",
    "\n",
    "    return max_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action_eps(q_table, obs, env, eps):\n",
    "    \n",
    "    # q_table[obs, a]\n",
    "    \n",
    "    if random.random() < eps:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return choose_action_greedy(q_table, obs, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_game(env, q_table, n_games=N_GAMES):\n",
    "    \n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action greedily\n",
    "            new_obs, reward, done, info = env.step(choose_action_greedy(q_table, discretize_state(obs), env))\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = new_obs\n",
    "\n",
    "    return total_reward/n_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(q_table, obs, action, reward, new_obs, n_actions, lr, gamma):\n",
    "    \"\"\"\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    gamma : float\n",
    "        Discount factor for future rewards\n",
    "    \"\"\"\n",
    "    \n",
    "    # TD(0) learning\n",
    "\n",
    "    # Update entry using bellman's equation\n",
    "#     q_table[(obs, action)] += lr * (\n",
    "#         reward + \n",
    "#         gamma*max([q_table[(new_obs, a)] for a in range(n_actions)]) -\n",
    "#         q_table[(obs, action)])\n",
    "    \n",
    "    \n",
    "    target = reward + gamma * max([q_table[(new_obs, a)] for a in range(n_actions)]) \n",
    "    \n",
    "    q_error = target - q_table[(obs, action)]\n",
    "    \n",
    "    q_table[(obs, action)] += lr * q_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_DECAY = 0.99993\n",
    "GAMMA = 0.99\n",
    "LR = 0.8\n",
    "\n",
    "EVALUATE_EVERY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(env, q_table, n_episodes, eps, eps_decay=EPS_DECAY, evaluate_every=EVALUATE_EVERY):\n",
    "    \n",
    "    mean_rewards = list()\n",
    "    \n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        done = False\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            # Choose an action epsilon-greedily\n",
    "            action = choose_action_eps(q_table, discretize_state(obs), env, eps)\n",
    "\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            print(discretize_state(obs))\n",
    "\n",
    "            q_learning(q_table, discretize_state(obs), action, reward, discretize_state(new_obs), n_actions, lr=LR, gamma=GAMMA)\n",
    "            obs = new_obs\n",
    "\n",
    "        eps *= eps_decay\n",
    "\n",
    "        # Evaluate policy every N games\n",
    "        if (i+1)%evaluate_every == 0:\n",
    "\n",
    "            test_reward = test_game(env, q_table)\n",
    "            print(f'\\tEp: {i+1}  Average reward: {total_reward/evaluate_every} Test reward: {test_reward} {eps:.2f}')\n",
    "            \n",
    "            total_reward = 0\n",
    "\n",
    "            mean_rewards.append(test_reward)\n",
    "            \n",
    "    return mean_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEp: 1000  Total reward: -200000.0 Test reward: -200.0 0.93\n",
      "\tEp: 2000  Total reward: -400000.0 Test reward: -200.0 0.87\n",
      "\tEp: 3000  Total reward: -600000.0 Test reward: -200.0 0.81\n",
      "\tEp: 4000  Total reward: -800000.0 Test reward: -200.0 0.76\n",
      "\tEp: 5000  Total reward: -1000000.0 Test reward: -200.0 0.70\n",
      "\tEp: 6000  Total reward: -1200000.0 Test reward: -200.0 0.66\n",
      "\tEp: 7000  Total reward: -1400000.0 Test reward: -200.0 0.61\n",
      "\tEp: 8000  Total reward: -1600000.0 Test reward: -200.0 0.57\n",
      "\tEp: 9000  Total reward: -1800000.0 Test reward: -200.0 0.53\n",
      "\tEp: 10000  Total reward: -2000000.0 Test reward: -200.0 0.50\n",
      "\tEp: 11000  Total reward: -2200000.0 Test reward: -200.0 0.46\n",
      "\tEp: 12000  Total reward: -2400000.0 Test reward: -200.0 0.43\n",
      "\tEp: 13000  Total reward: -2600000.0 Test reward: -200.0 0.40\n",
      "\tEp: 14000  Total reward: -2800000.0 Test reward: -200.0 0.38\n",
      "\tEp: 15000  Total reward: -3000000.0 Test reward: -200.0 0.35\n",
      "\tEp: 16000  Total reward: -3200000.0 Test reward: -200.0 0.33\n",
      "\tEp: 17000  Total reward: -3400000.0 Test reward: -200.0 0.30\n",
      "\tEp: 18000  Total reward: -3600000.0 Test reward: -200.0 0.28\n",
      "\tEp: 19000  Total reward: -3800000.0 Test reward: -200.0 0.26\n",
      "\tEp: 20000  Total reward: -4000000.0 Test reward: -200.0 0.25\n",
      "\tEp: 21000  Total reward: -4200000.0 Test reward: -200.0 0.23\n",
      "\tEp: 22000  Total reward: -4400000.0 Test reward: -200.0 0.21\n",
      "\tEp: 23000  Total reward: -4600000.0 Test reward: -200.0 0.20\n",
      "\tEp: 24000  Total reward: -4799952.0 Test reward: -200.0 0.19\n",
      "\tEp: 25000  Total reward: -4999952.0 Test reward: -200.0 0.17\n",
      "\tEp: 26000  Total reward: -5199952.0 Test reward: -200.0 0.16\n",
      "\tEp: 27000  Total reward: -5399952.0 Test reward: -200.0 0.15\n",
      "\tEp: 28000  Total reward: -5599952.0 Test reward: -200.0 0.14\n",
      "\tEp: 29000  Total reward: -5799952.0 Test reward: -200.0 0.13\n",
      "\tEp: 30000  Total reward: -5999952.0 Test reward: -200.0 0.12\n"
     ]
    }
   ],
   "source": [
    "# Main learning loop\n",
    "\n",
    "N_EPISODES = 30000\n",
    "\n",
    "eps = 1.0\n",
    "\n",
    "q_table = defaultdict(float)\n",
    "\n",
    "# Save n of actions\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "mean_rewards = training(env, q_table, N_EPISODES, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAIYCAYAAACxC+3YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdf7SuZV0n/vdHjpCShgaE8kOwsBHUTLdm37KYREG/CtrSBpuUdEbCZd+Vjn2HQSYzJ6casxr16w+cNDUVyZ80SgqOmWWo+xCggOhRMY6QHDVEQ2kd+Hz/eO6TT5u99zm49+HaHF6vtZ517ue6rvt+Ps/e93rWed77uu67ujsAAAAAI9xpdAEAAADAHZdgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAwJpU1TFVtXV0HXdkVXVYVX2rqvYaXQsA3FqCCQBYRlVdWVX/XFX7L2m/qKq6qg4fUxm7W1X9ZVV9p6oOnWs7tqquXGWfrqofmbZfVFV/uptrvLKqjt3xvLv/vru/v7tv2p2vCwC7g2ACAFb2xSRP3fGkqh6Y5C7jyvmuqtp0R3rd71XNfC//3/mnJL+x3vXsitvbzxgA1kowAQAre3OSp889PznJm+YHVNU+VfX7VfX3VfWVqnpNVd1l6rtHVf3vqtpWVf84bR8yt+9fVtV/q6q/qapvVtUHl87QmBt7TFVtrarTquofkrxhan/8NIvjuqr6WFU9aGp/RlX9+dz+W6rq7LnnV1XVg6ft/zk9v76qNlfVI+fGvaiq3lFVf1pV1yf55aq6S1X9yfSeLkvysCW1nlZVX57e0xVV9agV3tNqP7vLq+rxc2M3VdVXq+oh0/NHTO/3uqq6uKqOWfJzfUlV/U2SG5I8v6o2L3nt51fVe5ara/LyJE/dMQtiV1XV8UlekOTfTUsrLp7af6Cq/riqrpl+Nr+9Y9lFVf3ydA78YVV9PcmLquqHq+r/VNXXpvf9lqrabxr/5iSHJfnz6TX+c1UdPs3a2DSNuXdVnVNVX59+98+aq/FFVXV2Vb1p+h1dWlULc/279PsDgPUimACAlV2Q5O5Vdf/pS+S/S7J0iv7vJblfkgcn+ZEkByd54dR3p8wChPtk9kXy20leuWT/X0zyjCQHJtk7ya+vUs9BSe45He+U6Uv665P8SpIfTPLaJOdU1T5JPpLkkVV1p6q6V5I7J/mpJKmq+yb5/iSXTMf95FT/PZO8NcmfVdX3zb3uiUnekWS/JG9J8ptJfnh6HJdZYJPp2D+a5FeTPKy77zb1X7nC+1ntZ/e2zM1WmY7z1e6+sKoOTvK+JL891fzrSd5ZVQfMjX9aklOS3C2zkOGIqrr/XP8vZRY8reTLSV6X5EWrjLmF7v6LJP89ydunpRU/NnW9Mcn26X3+eJLHJPmPc7v+RJIvZHYevCRJJfmdJPdOcv8kh+6opbufluTvkzxheo3/sUwpb0uyddr/yUn++5KA4YQkZ2X2Oz0n03l5K39/ALAuBBMAsLodsyYeneQzmX1hTTJbJpDkWUme191f7+5vZval9KQk6e6vdfc7u/uGqe8lSX52yfHf0N2f7e5vJzk7sy/pK7k5yW92943T+GcleW13f7y7b+ruNya5MckjuvsLSb45He9nk3wgyZer6t9Mzz/a3TdPdf7pVOv27n5Zkn2S/Ojc6/5td7+nu2+eXvcXkrxkes9XZfbFf4ebpv2Pqqo7d/eV3f35pW9kZz+7zAKSE6rqrtPzX5zaklmo8P7ufv9U03lJFpM8bu4l/qS7L53e041J3j7tl6o6OsnhSf73Kj/rZBYMPGEa/z2rqh9K8tgkz+3uf+rua5P84dx7TZKru/sVU73f7u4t3X3e9LveluQPcstzZ6XXOzTJTyc5rbu/090XJflfmYU1O/z19PO7KbNzfEeAsku/PwBYT9YwAsDq3pzkr5IckSXLOJIckOSuSTbPvmcnmf2le8cU/btm9gX0+CT3mPrvVlV7zV2k8B/mjndDZjMZVrKtu78z9/w+SU6uqv9nrm3vzP5KnsxmTRyT2V/pP5Lkusy+3P7k9DxTnc/P7K/3907SSe6eZH5JyVVL6rj3krYv7djo7i1V9dzM/rp/dFV9IMl/6u6rlxxj1Z/ddJzLMwsG/jyzv/D/+Nz7fkpVPWHueHdO8uFVan5jkrdV1X/N7Av62VNgsaLu3lZVr0zy4iSvXm3sTtxnqu+aufd6pyU1/qt6q+rAzAKfR2Y26+NOSf5xF1/v3kl2hD07fCnJwtzzpefd91XVplvx+wOAdWPGBACsoru/lNlFMB+X5F1Lur+a2fKMo7t7v+nxA929I1x4fmYzD36iu++e5Gem9sr3ppc8vyqzmQv7zT3u2t1vm/p3BBOPnLY/klkw8bPTdqbrSZyW2SyIe3T3fkm+saTGpa97TWZLC3Y47F8V2f3W7v7pzL6Qd2ZLNpba2c8u+e5yjhOTXNbdW+be95uXvO99u/t3V6q5uy9I8s/Tz+IXs/oyjnkvTfJvkzx0F8ff4rWnem9Msv9cvXfv7qNX2ed3prYHTefOL2X138m8q5Pcs6ruNtd2WOZm+6xa/K79/gBg3QgmAGDn/kOSn+vuf5pvnJZCvC7JH05/4U5VHVxVx01D7pbZl+/rquqemV2bYT29LsmpVfUTNbNvVf3fc19IP5LZl+q7dPfWJB/NbPbGDyb5u7katyfZlmRTVb0wsxkTqzk7yek1u7jnIUn+ZcZGVf1oVf3cdJ2L72T2/m9xC8td+Nkls2sgPCbJs/PdZRzJ7DofT6iq46pqr6r6vppdHPSQrO5NmV1LYXt3//VOxu6o87okL0vyn3dl/OQrSQ6v6W4g3X1Nkg8meVlV3X267scPV9VqSzPuluRbmZ07Byf5f5d5jfuuUPNVST6W5Hemn82DMjuH37Kzwnf19wcA60kwAQA70d2f7+7FFbpPS7IlyQU1u2vF+fnu9Rn+KLPbi341swtp/sU617WY2XUaXpnZNP8tSX55rv+zmX25/ej0/PrMLrD4N3NLST6Q5Nwkn81suv93cstlEEv91jT2i5l94Z6ffbBPkt/N7D3/Q2YXc3zBCsdZ7We34wv93yb5vzK7RsSO9qsym0XxgswClasy++K+s//XvDnJA7LrsyV2+J+5dV/O/2z692tVdeG0/fTMltlcltnv6h1J7rXKMX4ryUMym73yvtxyts7vJPmvNbsryXIXTH1qZtfRuDrJuzO7Nsl5u1D7rfn9AcC6qO7VZgICAOwZanYr0muTPKS7Pze6HgBgxowJAOCO4tlJPimUAICNxV05AIA9XlVdmdnFI584uBQAYAlLOQAAAIBhLOUAAAAAhhFMAAAAAMPsUdeY2H///fvwww8fXQYAAACwxObNm7/a3Qcsbd+jgonDDz88i4sr3WYeAAAAGKWqvrRcu6UcAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBh1hxMVNVTqurSqrq5qhbm2veuqjdU1aeq6uKqOmau76FT+5aqenlV1TLHralvS1VdUlUPWWutAAAAwMayHjMmPp3k55P81ZL2ZyVJdz8wyaOTvKyqdrzeq5OckuTI6XH8Msd97Fz/KdM+AAAAwB5kzcFEd1/e3Vcs03VUkg9NY65Ncl2Shaq6V5K7d/ffdncneVOSJy6z/4lJ3tQzFyTZb9oXAAAA2EPszmtMXJzkxKraVFVHJHlokkOTHJxk69y4rVPbUgcnuWoXxgEAAAC3U5t2ZVBVnZ/koGW6zuju966w2+uT3D/JYpIvJflYku1JbnE9iSS93MvuyriqOiWzpR457LDDVigFAAAA2Ih2KZjo7mNv7YG7e3uS5+14XlUfS/K5JP+Y5JC5oYckuXqZQ2zNbIbFquO6+8wkZybJwsLCcgEHAAAAsEHttqUcVXXXqtp32n50ku3dfVl3X5Pkm1X1iOluHE9Pstysi3OSPH26O8cjknxj2hcAAADYQ+zSjInVVNWTkrwiyQFJ3ldVF3X3cUkOTPKBqro5yZeTPG1ut2cn+ZMkd0ly7vRIVZ2aJN39miTvT/K4JFuS3JDkGWutFQAAANhYanZjjD3DwsJCLy4uji4DAAAAWKKqNnf3wtL23XlXDgAAAIBVCSYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADLOmYKKqnlJVl1bVzVW1MNe+d1W9oao+VVUXV9UxU/tdq+p9VfWZab/fXeG4h1fVt6vqounxmrXUCQAAAGxMm9a4/6eT/HyS1y5pf1aSdPcDq+rAJOdW1cOmvt/v7g9X1d5JPlRVj+3uc5c59ue7+8FrrA8AAADYwNY0Y6K7L+/uK5bpOirJh6Yx1ya5LslCd9/Q3R+e2v85yYVJDllLDQAAAMDt1+66xsTFSU6sqk1VdUSShyY5dH5AVe2X5AmZAoxlHFFVf1dVH6mqR670QlV1SlUtVtXitm3b1qt+AAAA4Daw06UcVXV+koOW6Tqju9+7wm6vT3L/JItJvpTkY0m2zx1zU5K3JXl5d39hmf2vSXJYd3+tqh6a5D1VdXR3X790YHefmeTMJFlYWOidvR8AAABg49hpMNHdx97ag3b39iTP2/G8qj6W5HNzQ85M8rnu/qMV9r8xyY3T9uaq+nyS+2UWdAAAAAB7iN2ylGO6+8a+0/ajk2zv7sum57+d5AeSPHeV/Q+oqr2m7fsmOTLJcjMrAAAAgNuxtd4u9ElVtTXJTyZ5X1V9YOo6MMmFVXV5ktOSPG0af0iSMzK7OOaF061A/+PUd0JVvXja/2eSXFJVFyd5R5JTu/vra6kVAAAA2Hiqe8+5LMPCwkIvLlrtAQAAABtNVW3u7oWl7bvrrhwAAAAAOyWYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADDMmoKJqnpKVV1aVTdX1cJc+95V9Yaq+lRVXVxVx8z1/WVVXVFVF02PA1c49ulVtWUae9xa6gQAAAA2pk1r3P/TSX4+yWuXtD8rSbr7gVPwcG5VPay7b576/313L6500Ko6KslJSY5Ocu8k51fV/br7pjXWCwAAAGwga5ox0d2Xd/cVy3QdleRD05hrk1yXZGGZcSs5MclZ3X1jd38xyZYkD19LrQAAAMDGs7uuMXFxkhOralNVHZHkoUkOnet/w7SM4zeqqpbZ/+AkV8093zq13UJVnVJVi1W1uG3btvWqHwAAALgN7HQpR1Wdn+SgZbrO6O73rrDb65PcP8liki8l+ViS7VPfv+/uL1fV3ZK8M8nTkrxp6csuc8xe7oW6+8wkZybJwsLCsmMAAACAjWmnwUR3H3trD9rd25M8b8fzqvpYks9NfV+e/v1mVb01syUaS4OJrfnXMywOSXL1ra0DAAAA2Nh2y1KOqrprVe07bT86yfbuvmxa2rH/1H7nJI/P7AKaS52T5KSq2mdaCnJkkk/sjloBAACAcdZ0V46qelKSVyQ5IMn7quqi7j4uyYFJPlBVNyf5cmbLNZJkn6n9zkn2SnJ+ktdNxzohyUJ3v7C7L62qs5NcltkSkOe4IwcAAADseap7z7ksw8LCQi8urngXUgAAAGCQqtrc3be4Y+fuuisHAAAAwE4JJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhmTcFEVT2lqi6tqpuramGufe+qekNVfaqqLq6qY6b2u1XVRXOPr1bVHy1z3MOr6ttz416zljoBAACAjWnTGvf/dJKfT/LaJe3PSpLufmBVHZjk3Kp6WHd/M8mDdwyqqs1J3rXCsT/f3Q9eoQ8AAADYA6xpxkR3X97dVyzTdVSSD01jrk1yXZKF+QFVdWSSA5N8dC01AAAAALdfu+saExcnObGqNlXVEUkemuTQJWOemuTt3d0rHOOIqvq7qvpIVT1yN9UJAAAADLTTpRxVdX6Sg5bpOqO737vCbq9Pcv8ki0m+lORjSbYvGXNSkqetsP81SQ7r7q9V1UOTvKeqju7u65ep75QkpyTJYYcdtrO3AwAAAGwgOw0muvvYW3vQ7t6e5Hk7nlfVx5J8bu75jyXZ1N2bV9j/xiQ3Ttubq+rzSe6XWdCxdOyZSc5MkoWFhZVmXwAAAAAb0G5ZylFVd62qfaftRyfZ3t2XzQ15apK3rbL/AVW117R93yRHJvnC7qgVAAAAGGdNd+WoqicleUWSA5K8r6ou6u7jMruo5Qeq6uYkX84tl2z8QpLHLTnWCUkWuvuFSX4myYuranuSm5Kc2t1fX0utAAAAwMZTK1978vZnYWGhFxdvsdoDAAAAGKyqNnf3wtL23XVXDgAAAICdEkwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGGbNwURVvbSqPlNVl1TVu6tqv7m+06tqS1VdUVXHzbUfP7Vtqar/ssJx96mqt09jPl5Vh6+1VgAAAGBjWY8ZE+cleUB3PyjJZ5OcniRVdVSSk5IcneT4JK+qqr2qaq8k/1+SxyY5KslTp7FL/Yck/9jdP5LkD5P83jrUCgAAAGwgm9Z6gO7+4NzTC5I8edo+MclZ3X1jki9W1ZYkD5/6tnT3F5Kkqs6axl625NAnJnnRtP2OJK+squruXmvNG8lv/fmluezq60eXAQAAwO3EUfe+e37zCUePLmPdrPc1Jp6Z5Nxp++AkV831bZ3aVmpf6l/Gdff2JN9I8oNLB1XVKVW1WFWL27ZtW/MbAAAAAG47uzRjoqrOT3LQMl1ndPd7pzFnJNme5C07dltmfGf5MGS5WRAr7f+vG7rPTHJmkiwsLNzuZlPsSSkXAAAA3Fq7FEx097Gr9VfVyUken+RRc0sttiY5dG7YIUmunrZXap+3Y/+tVbUpyQ8k+fqu1AsAAADcPqzHXTmOT3JakhO6+4a5rnOSnDTdXeOIJEcm+USSTyY5sqqOqKq9M7tA5jnLHPqcJCdP209O8n/2tOtLAAAAwB3dmi9+meSVSfZJcl5VJckF3X1qd19aVWdndlHL7Ume0903JUlV/WqSDyTZK8nru/vSqf3FSRa7+5wkf5zkzdNFM7+eWYABAAAA7EFqT5qEsLCw0IuLi6PLAAAAAJaoqs3dvbC0fb3vygEAAACwywQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADLOmYKKqXlpVn6mqS6rq3VW131zf6VW1paquqKrjprZDq+rDVXV5VV1aVb+2wnGPqapvVNVF0+OFa6kTAAAA2JjWOmPivCQP6O4HJflsktOTpKqOSnJSkqOTHJ/kVVW1V5LtSZ7f3fdP8ogkz5nGLuej3f3g6fHiNdYJAAAAbEBrCia6+4PdvX16ekGSQ6btE5Oc1d03dvcXk2xJ8vDuvqa7L5z2/WaSy5McvJYaAAAAgNuv9bzGxDOTnDttH5zkqrm+rVkSQFTV4Ul+PMnHVzjeT1bVxVV1blUdvY51AgAAABvEpp0NqKrzkxy0TNcZ3f3eacwZmS3TeMuO3ZYZ33PH/P4k70zy3O6+fpmxFya5T3d/q6oel+Q9SY5cob5TkpySJIcddtjO3g4AAACwgew0mOjuY1frr6qTkzw+yaO6e0f4sDXJoXPDDkly9TT+zpmFEm/p7net8JrXz22/v6peVVX7d/dXlxl7ZpIzk2RhYaGX9gMAAAAb11rvynF8ktOSnNDdN8x1nZPkpKrap6qOyGy2wyeqqpL8cZLLu/sPVjnuQdPYVNXDpzq/tpZaAQAAgI1npzMmduKVSfZJct6UI1zQ3ad296VVdXaSyzJb4vGc7r6pqn46ydOSfKqqLpqO8YJpVsSpSdLdr0ny5CTPrqrtSb6d5KS52RgAAADAHqL2pO/7CwsLvbi4OLoMAAAAYImq2tzdC0vb1/OuHAAAAAC3imACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDBrDiaq6qVV9ZmquqSq3l1V+831nV5VW6rqiqo6bq79yqr6VFVdVFWLKxy3qurl0/6XVNVD1lorAAAAsLGsx4yJ85I8oLsflOSzSU5Pkqo6KslJSY5OcnySV1XVXnP7/dvufnB3L6xw3McmOXJ6nJLk1etQKwAAALCBrDmY6O4Pdvf26ekFSQ6Ztk9MclZ339jdX0yyJcnDb8WhT0zypp65IMl+VXWvtdYLAAAAbBzrfY2JZyY5d9o+OMlVc31bp7Yk6SQfrKrNVXXKCsdabf9/UVWnVNViVS1u27ZtTcUDAAAAt61NuzKoqs5PctAyXWd093unMWck2Z7kLTt2W2Z8T//+VHdfXVUHJjmvqj7T3X+19GVX2f+7Dd1nJjkzSRYWFm7RDwAAAGxcuxRMdPexq/VX1clJHp/kUd29IxzYmuTQuWGHJLl6Ot6Of6+tqndntsRjaTCx4v4AAADAnmE97spxfJLTkpzQ3TfMdZ2T5KSq2qeqjsjsIpafqKp9q+pu0777JnlMkk8vc+hzkjx9ujvHI5J8o7uvWWu9AAAAwMaxSzMmduKVSfbJbElGklzQ3ad296VVdXaSyzJb4vGc7r6pqn4oybunsZuSvLW7/yJJqurUJOnu1yR5f5LHZXbRzBuSPGMdagUAAAA2kPruyovbv4WFhV5cXBxdBgAAALBEVW3u7oWl7et9Vw4AAACAXSaYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwgjmSPNMAAArfSURBVAkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADDMmoKJqnppVX2mqi6pqndX1X5zfadX1ZaquqKqjpvafrSqLpp7XF9Vz13muMdU1Tfmxr1wLXUCAAAAG9OmNe5/XpLTu3t7Vf1ektOTnFZVRyU5KcnRSe6d5Pyqul93X5HkwUlSVXsl+XKSd69w7I929+PXWB8AAACwga1pxkR3f7C7t09PL0hyyLR9YpKzuvvG7v5iki1JHr5k90cl+Xx3f2ktNQAAAAC3X+t5jYlnJjl32j44yVVzfVuntnknJXnbKsf7yaq6uKrOraqjVxpUVadU1WJVLW7btu17qRsAAAAYZKfBRFWdX1WfXuZx4tyYM5JsT/KWHU3LHKrnxu+d5IQkf7bCy16Y5D7d/WNJXpHkPSvV191ndvdCdy8ccMABO3s7AAAAwAay02tMdPexq/VX1clJHp/kUd29I3zYmuTQuWGHJLl67vljk1zY3V9Z4TWvn9t+f1W9qqr27+6v7qxeAAAA4PZjrXflOD7JaUlO6O4b5rrOSXJSVe1TVUckOTLJJ+b6n5pVlnFU1UFVVdP2w6c6v7aWWgEAAICNZ6135Xhlkn2SnDflCBd096ndfWlVnZ3kssyWeDynu29Kkqq6a5JHJ/mV+QNV1alJ0t2vSfLkJM+uqu1Jvp3kpLnZGAAAAMAeovak7/sLCwu9uLg4ugwAAABgiara3N0LS9vX864cAAAAALeKYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwjGACAAAAGEYwAQAAAAwjmAAAAACGEUwAAAAAwwgmAAAAgGEEEwAAAMAwggkAAABgGMEEAAAAMIxgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBh1hxMVNVLq+ozVXVJVb27qvab2n+wqj5cVd+qqlcu2eehVfWpqtpSVS+vqlrmuDX1bZmO/ZC11goAAABsLOsxY+K8JA/o7gcl+WyS06f27yT5jSS/vsw+r05ySpIjp8fxy4x57Fz/KdM+AAAAwB5kzcFEd3+wu7dPTy9IcsjU/k/d/deZBRT/oqruleTu3f233d1J3pTkicsc+sQkb+qZC5LsN+0LAAAA7CHW+xoTz0xy7k7GHJxk69zzrVPbcuOu2oVxAAAAwO3Upl0ZVFXnJzloma4zuvu905gzkmxP8padHW6Ztv5ex1XVKZkt9chhhx22k5cGAAAANpJdCia6+9jV+qvq5CSPT/KoaXnGarZmWu4xOSTJ1SuMO3Rn47r7zCRnJsnCwsLOXhsAAADYQNbjrhzHJzktyQndfcPOxnf3NUm+WVWPmO7G8fQk711m6DlJnj7dneMRSb4x7QsAAADsIXZpxsROvDLJPknOm+76eUF3n5okVXVlkrsn2buqnpjkMd19WZJnJ/mTJHfJ7JoU507jT02S7n5NkvcneVySLUluSPKMdagVAAAA2EDWHEx094+s0nf4Cu2LSR6wTPtr5rY7yXPWWh8AAACwca33XTkAAAAAdplgAgAAABhGMAEAAAAMI5gAAAAAhhFMAAAAAMMIJgAAAIBhBBMAAADAMIIJAAAAYBjBBAAAADCMYAIAAAAYRjABAAAADCOYAAAAAIYRTAAAAADDCCYAAACAYQQTAAAAwDCCCQAAAGAYwQQAAAAwTHX36BrWTVVtS/Kl0XV8D/ZP8tXRRXCH4pxjBOcdtzXnHCM477itOecY4Xs97+7T3Qcsbdyjgonbq6pa7O6F0XVwx+GcYwTnHbc15xwjOO+4rTnnGGG9zztLOQAAAIBhBBMAAADAMIKJjeHM0QVwh+OcYwTnHbc15xwjOO+4rTnnGGFdzzvXmAAAAACGMWMCAAAAGEYwMVBVHV9VV1TVlqr6L6Pr4Y6hqq6sqk9V1UVVtTi6HvZMVfX6qrq2qj4913bPqjqvqj43/XuPkTWyZ1nhnHtRVX15+ry7qKoeN7JG9ixVdWhVfbiqLq+qS6vq16Z2n3XsNqucdz7v2C2q6vuq6hNVdfF0zv3W1H5EVX18+qx7e1XtvabXsZRjjKraK8lnkzw6ydYkn0zy1O6+bGhh7PGq6sokC93tftfsNlX1M0m+leRN3f2Aqe1/JPl6d//uFMbeo7tPG1kne44VzrkXJflWd//+yNrYM1XVvZLcq7svrKq7Jdmc5IlJfjk+69hNVjnvfiE+79gNqqqS7Nvd36qqOyf56yS/luQ/JXlXd59VVa9JcnF3v/p7fR0zJsZ5eJIt3f2F7v7nJGclOXFwTQDrorv/KsnXlzSfmOSN0/YbM/uPFKyLFc452G26+5ruvnDa/maSy5McHJ917EarnHewW/TMt6and54eneTnkrxjal/zZ51gYpyDk1w193xrfKhw2+gkH6yqzVV1yuhiuEP5oe6+Jpn9xyrJgYPr4Y7hV6vqkmmphyn17BZVdXiSH0/y8fis4zay5LxLfN6xm1TVXlV1UZJrk5yX5PNJruvu7dOQNX+XFUyMU8u0WVfDbeGnuvshSR6b5DnT9GeAPdGrk/xwkgcnuSbJy8aWw56oqr4/yTuTPLe7rx9dD3cMy5x3Pu/Ybbr7pu5+cJJDMpv5f//lhq3lNQQT42xNcujc80OSXD2oFu5Auvvq6d9rk7w7sw8XuC18ZVobu2ON7LWD62EP191fmf4zdXOS18XnHetsWm/9ziRv6e53Tc0+69itljvvfN5xW+ju65L8ZZJHJNmvqjZNXWv+LiuYGOeTSY6crma6d5KTkpwzuCb2cFW173ShpFTVvkkek+TTq+8F6+acJCdP2ycnee/AWrgD2PHlcPKk+LxjHU0XhPvjJJd39x/MdfmsY7dZ6bzzecfuUlUHVNV+0/Zdkhyb2bVNPpzkydOwNX/WuSvHQNNtfP4oyV5JXt/dLxlcEnu4qrpvZrMkkmRTkrc679gdquptSY5Jsn+SryT5zSTvSXJ2ksOS/H2Sp3S3ixWyLlY4547JbFpzJ7kyya/sWPsPa1VVP53ko0k+leTmqfkFma3391nHbrHKeffU+LxjN6iqB2V2ccu9MpvYcHZ3v3j6XnFWknsm+bskv9TdN37PryOYAAAAAEaxlAMAAAAYRjABAADA/9+OHQsAAAAADPK3Hsaewgg2YgIAAADYiAkAAABgIyYAAACAjZgAAAAANmICAAAA2IgJAAAAYBMOImKqoj8UlwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(mean_rewards)\n",
    "plt.title(\"Mean rewards every N Iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/miniconda3/envs/rl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-96d18f03e4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Run Q-learning algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m# Plot Rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-96d18f03e4b0>\u001b[0m in \u001b[0;36mQLearning\u001b[0;34m(env, learning, discount, epsilon, min_eps, episodes)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Initialize Q table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Import and initialize Mountain Car Environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "\n",
    "# Define Q-learning function\n",
    "def QLearning(env, learning, discount, epsilon, min_eps, episodes):\n",
    "    # Determine size of discretized state space\n",
    "    num_states = (env.observation_space.high - env.observation_space.low)*\\\n",
    "                    np.array([10, 100])\n",
    "    num_states = np.round(num_states, 0).astype(int) + 1\n",
    "    \n",
    "    print(num_states)\n",
    "    \n",
    "    1/0\n",
    "    \n",
    "    # Initialize Q table\n",
    "    Q = np.random.uniform(low = -1, high = 1, \n",
    "                          size = (num_states[0], num_states[1], \n",
    "                                  env.action_space.n))\n",
    "    \n",
    "    # Initialize variables to track rewards\n",
    "    reward_list = []\n",
    "    ave_reward_list = []\n",
    "    \n",
    "    # Calculate episodic reduction in epsilon\n",
    "    reduction = (epsilon - min_eps)/episodes\n",
    "    \n",
    "    # Run Q learning algorithm\n",
    "    for i in range(episodes):\n",
    "        # Initialize parameters\n",
    "        done = False\n",
    "        tot_reward, reward = 0,0\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Discretize state\n",
    "        state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "        state_adj = np.round(state_adj, 0).astype(int)\n",
    "    \n",
    "        while done != True:   \n",
    "            # Render environment for last five episodes\n",
    "            if i >= (episodes - 20):\n",
    "                env.render()\n",
    "                \n",
    "            # Determine next action - epsilon greedy strategy\n",
    "            if np.random.random() < 1 - epsilon:\n",
    "                action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "            else:\n",
    "                action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "            # Get next state and reward\n",
    "            state2, reward, done, info = env.step(action) \n",
    "            \n",
    "            # Discretize state2\n",
    "            state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "            state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "            \n",
    "            #Allow for terminal states\n",
    "            if done and state2[0] >= 0.5:\n",
    "                Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "            # Adjust Q value for current state\n",
    "            else:\n",
    "                delta = learning*(reward + \n",
    "                                 discount*np.max(Q[state2_adj[0], \n",
    "                                                   state2_adj[1]]) - \n",
    "                                 Q[state_adj[0], state_adj[1],action])\n",
    "                Q[state_adj[0], state_adj[1],action] += delta\n",
    "                                     \n",
    "            # Update variables\n",
    "            tot_reward += reward\n",
    "            state_adj = state2_adj\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > min_eps:\n",
    "            epsilon -= reduction\n",
    "        \n",
    "        # Track rewards\n",
    "        reward_list.append(tot_reward)\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            ave_reward = np.mean(reward_list)\n",
    "            ave_reward_list.append(ave_reward)\n",
    "            reward_list = []\n",
    "            \n",
    "        if (i+1) % 100 == 0:    \n",
    "            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return ave_reward_list\n",
    "\n",
    "# Run Q-learning algorithm\n",
    "rewards = QLearning(env, 0.2, 0.9, 0.8, 0, 5000)\n",
    "\n",
    "# Plot Rewards\n",
    "plt.plot(100*(np.arange(len(rewards)) + 1), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "# plt.savefig('rewards.jpg')   \n",
    "plt.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
